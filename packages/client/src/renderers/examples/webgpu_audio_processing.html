<html lang='en'>
<head>
  <title>WebGPU - Audio Processing</title>
  <meta charset='utf-8'>
  <meta content='width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0' name='viewport'>
  <link href='styles.css' rel='stylesheet' type='text/css'>
</head>
<body>
<div id='overlay'>
  <button id='startButton'>Play</button>
</div>
<div id='info'>
  <a href='https://threejs.org' rel='noopener' target='_blank'>three.js</a> WebGPU - Audio Processing
  <br>Click on screen to process the audio using WebGPU.
</div>
<script type='module'>
  import * as THREE from 'three';
  import { ShaderNode, uniform, storage, instanceIndex, float, texture, viewportTopLeft, color } from 'three/nodes';

  import { GUI } from 'three/examples/jsm/libs/lil-gui.module.min.js';
  import { createRenderer } from '@/renderers/webgpu/createRenderer.ts';
  import { PerspectiveCamera } from '@/renderers/webgpu/core/camera/PerspectiveCamera.ts';
  import { Scene } from '@/renderers/webgpu/core/scenes/Scene.ts';
  import { DataTexture } from '@/renderers/webgpu/core/textures/DataTexture.ts';
  import { containRenderer, createResize } from '@/renderers/examples/patterns/patterns.ts';
  import { InstancedBufferAttribute } from '@/renderers/webgpu/core/InstancedBufferAttribute.ts';

  const analyserBuffer = new Uint8Array(1024);


  const sound = await fetch('sounds/webgpu-audio-processing.mp3');
  const soundBuffer = await sound.arrayBuffer();
  const audioContext = new AudioContext();
  const audioBuffer = await audioContext.decodeAudioData(soundBuffer);

  const waveBuffer = audioBuffer.getChannelData(0);
  const sampleRate = audioBuffer.sampleRate / audioBuffer.numberOfChannels;
  const waveGPUBuffer = new InstancedBufferAttribute(waveBuffer, 1);
  const waveStorageNode = storage(waveGPUBuffer, 'float', waveBuffer.length);
  const waveNode = storage(new InstancedBufferAttribute(waveBuffer, 1), 'float', waveBuffer.length);
  const pitch = uniform(1.5);
  const delayVolume = uniform(.2);
  const delayOffset = uniform(.55);

  const computeShaderNode = new ShaderNode((stack) => {
    const index = float(instanceIndex);
    // pitch
    const time = index.mul(pitch);
    let wave = waveNode.element(time);

    // delay
    for (let i = 1; i < 7; i++) {
      const waveOffset = waveNode.element(index.sub(delayOffset.mul(sampleRate).mul(i)).mul(pitch));
      const waveOffsetVolume = waveOffset.mul(delayVolume.div(i * i));

      wave = wave.add(waveOffsetVolume);
    }


    // store
    const waveStorageElementNode = waveStorageNode.element(instanceIndex);

    stack.assign(waveStorageElementNode, wave);

  });
  const computeNode = computeShaderNode.compute(waveBuffer.length);

  const gui = new GUI();
  gui.add(pitch, 'value', .5, 2, 0.01).name('pitch');
  gui.add(delayVolume, 'value', 0, 1, .01).name('delayVolume');
  gui.add(delayOffset, 'value', .1, 1, .01).name('delayOffset');


  // renderer
  console.log(PerspectiveCamera);
  const camera = new PerspectiveCamera(45, window.innerWidth / window.innerHeight, 0.01, 30);


  // nodes
  const analyserTexture = new DataTexture(analyserBuffer, analyserBuffer.length, 1, THREE.RedFormat);
  const spectrum = texture(analyserTexture, viewportTopLeft.x).x.mul(viewportTopLeft.y);
  const backgroundNode = color(0x0000FF).mul(spectrum);

  // scene
  const scene = new Scene();
  scene.backgroundNode = backgroundNode;

  let analyser = null;
  const renderer = createRenderer();
  renderer.setPixelRatio(window.devicePixelRatio);
  renderer.setSize(window.innerWidth, window.innerHeight);
  renderer.setAnimationLoop(() => {
    if (analyser) {
      analyser.getByteFrequencyData(analyserBuffer);
      analyserTexture.needsUpdate = true;
    }

    renderer.render(scene, camera);
  });

  window.onresize = createResize(renderer, camera);

  let audio;
  document.onclick = async () => {
    const overlay = document.getElementById('overlay');
    if (overlay) overlay.remove();
    if (audio) audio.stop();

    await renderer.compute(computeNode);
    const wave = new Float32Array(await renderer.getArrayBufferAsync(waveGPUBuffer));
    const context = new AudioContext({ sampleRate });

    audio = context.createBufferSource();
    audio.connect(context.destination);

    const buffer = context.createBuffer(1, wave.length, sampleRate);
    buffer.copyToChannel(wave, 0);
    audio.buffer = buffer;

    audio.start();

    analyser = context.createAnalyser();
    analyser.fftSize = 2048;
    audio.connect(analyser);
  };

  containRenderer(renderer);
</script>
</body>
</html>
